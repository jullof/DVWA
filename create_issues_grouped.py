#!/usr/bin/env python3
# create_issues_grouped.py — chunked GitHub issue creation to avoid 65k body limit

import os, sys, json, subprocess, textwrap

REPORT_DIR   = os.environ.get("REPORT_DIR", "reports")
ITEMS_PATH   = os.path.join(REPORT_DIR, "ai_findings.json")
REPO         = os.environ.get("GITHUB_REPO")
GH_TOKEN     = os.environ.get("GH_TOKEN") or os.environ.get("GITHUB_TOKEN")

MAX_CHARS            = int(os.environ.get("GH_ISSUE_BODY_LIMIT", "60000"))
MAX_ITEMS_PER_ISSUE  = int(os.environ.get("MAX_ITEMS_PER_ISSUE", "0")) 

SEV_RANK = {"critical":0, "high":1, "medium":2, "low":3}

def gh(*args, input_text=None):
    env = os.environ.copy()
    if GH_TOKEN:
        env["GH_TOKEN"] = GH_TOKEN
    cmd = ["gh"] + list(args)
    p = subprocess.run(
        cmd,
        input=input_text.encode("utf-8") if input_text else None,
        env=env,
        stdout=subprocess.PIPE, stderr=subprocess.PIPE
    )
    if p.returncode != 0:
        err = p.stderr.decode()
        print(f"[WARN] gh error: {err.strip()}", file=sys.stderr)
        return None
    return p.stdout.decode().strip()

def load_items(path):
    if not os.path.exists(path):
        print(f"[ERR] Missing: {path}", file=sys.stderr)
        return []
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def render_item(it, idx):
    title = it.get("title") or "Finding"
    sev   = (it.get("severity") or "medium").lower()
    pri   = it.get("priority") or "P3"
    where = (it.get("where") or "").strip()
    ev    = (it.get("evidence") or "").strip()
    if len(ev) > 200:
        ev = ev[:200] + "…"
    root  = (it.get("root_cause") or "").strip()
    rem   = it.get("recommended_remediation") or []
    rem_out = []
    for s in rem[:4]:
        s = str(s).strip()
        if len(s) > 160: s = s[:160] + "…"
        if s: rem_out.append(s)
    refs  = it.get("references") or []

    lines = []
    lines.append(f"## {idx}. {title}  *(severity:{sev}, priority:{pri})*")
    if where:
        lines.append(f"- **Where:** {where}")
    if ev:
        lines.append(f"- **Evidence:** `{ev}`")
    if root:
        lines.append(f"- **Root cause:** {root}")
    if rem_out:
        lines.append("- **Proposed remediation:**")
        for s in rem_out:
            lines.append(f"  - {s}")
    if refs:
        lines.append("- **References:**")
        for r in refs[:4]:
            if r:
                lines.append(f"  - {r}")
    lines.append("")  # spacer
    return "\n".join(lines)

def build_chunked_bodies(group, items):
    items = sorted(items, key=lambda x: (SEV_RANK.get((x.get("severity") or "medium").lower(), 2), x.get("title","")))
    chunks = []
    cur = []
    cur_text = ""
    base_hdr = f"# AI triage summary — {group.upper()}\n\n"

    def start_new():
        return base_hdr, []

    body, cur = start_new()
    idx_offset = 0
    for i, it in enumerate(items, 1):
        section = render_item(it, idx_offset + len(cur) + 1)
        if (len(body) + len(section)) > MAX_CHARS or (MAX_ITEMS_PER_ISSUE and len(cur) >= MAX_ITEMS_PER_ISSUE):
            chunks.append(body)
            body, cur = start_new()
            idx_offset += len(cur) 
            section = render_item(it, len(cur) + 1)
        body += section
        cur.append(it)

    if cur:
        chunks.append(body)

    return chunks

def main():
    if not (REPO and GH_TOKEN):
        print("[ERR] GITHUB_REPO or GH_TOKEN missing", file=sys.stderr)
        sys.exit(1)

    items = load_items(ITEMS_PATH)
    if not items:
        print("[WARN] No items to publish.")
        return

    # Group according to class
    groups = {}
    for it in items:
        g = (it.get("class") or "other").strip().lower()
        groups.setdefault(g, []).append(it)

    base_labels = ["security", "ai-triage"]

    for group, gitems in groups.items():
        if not gitems:
            continue

        chunks = build_chunked_bodies(group, gitems)
        total_chunks = len(chunks)

        for i, body in enumerate(chunks, 1):
            # Title
            num_items = sum(1 for line in body.splitlines() if line.startswith("## "))
            labels = ",".join(base_labels + [group])

            title = f"[Security][{group.upper()}] AI triage summary — Part {i}/{total_chunks} ({num_items} items)"
            meta = textwrap.dedent(f"""
            > Generated by AI triage. Group: **{group}** — Part **{i}/{total_chunks}**.  
            > Body truncated to respect GitHub 65k limit per issue.
            """).strip() + "\n\n"

            full_body = meta + body

            if len(full_body) > MAX_CHARS:
                full_body = full_body[:MAX_CHARS-50] + "\n\n*(truncated)*"

            url = gh("issue", "create",
                     "--repo", REPO,
                     "--title", title,
                     "--label", labels,
                     "--body-file", "-",
                     input_text=full_body)
            if url:
                print(url)

if __name__ == "__main__":
    main()
